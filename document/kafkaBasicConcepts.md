### Replication
* 각 메세지들을 여러 개로 복제해서 카프카 클러스터내 브로커들에게 분산시키는 동작을 의미
* 리플리케이션 동작 덕분에 하나의 브로커가 종료되더라도 카프카는 안정성을 유지할 수 있다.
* 토핑 생성 명령어 중 replication-factor 라는 옵션으로 설정할 수 있다.
* 예를 들어, 특정 토픽의 replication-factor 를 3으로 설정했다면, 해당 토픽의 파티션이 브로커로 리플리케이션이 된다.
* 리플리케이션 팩터 수가 커지면 안정성은 높아지지만, 그만큼 브로커 리소스를 많이 사용하게 된다.
* 추천 수
  * 테스트나 개발 환경 : 리플리케이션 팩터 수 1로 설정
  * 운영 환경 (유실 허용하지 않음) : 리플리케이션 팩터 수 3으로 설정

### Partition
* 하나의 토픽이 한 번에 처리할 수 있는 한계를 높이기 위해 토픽 하나를 여러개로 나눠 병렬 처리가 가능하게 만든 것을 파티션이라고 한다.
* 파티션을 여러개로 나누면 병렬 처리가 가능해진다.
* 파티션 수만큼 컨슈머를 연결할 수도 있다.
* 파티션 수는 초기 생성 후 언제든지 늘릴 수 있지만, 반대로 한 번 늘린 파티션은 절대로 줄일 수 없다.
  * 여러 이유가 있겠지만, 대표적인 이유로 다수 브로커에 분배되어 있는 세그먼트를 다시 재배열하는 것에 상당한 리소스가 들어가기 때문이다.
  * KIP-694(https://cwiki.apache.org/confluence/display/KAFKA/KIP-694%3A+Support+Reducing+Partitions+for+Topics)에서 파티션 개수를 줄이는 방안에 대해 논의했으나 더 이상 진행되고 있지는 않다.
  * 따라서, 초기에 생성할 때 파티션 수를 작게 설정하고 메세지 처리량이나 컨슈머 LAG 을 모니터링하면서 조금씩 늘려가는게 좋다.

### Segment
* 카프카 프로듀서에 브로커로 전송된 메세지는 토픽의 파티션에 저장되며, 각 메세지들은 세그먼트라는 로그 파일의 형태로 브로커의 로컬 디스크에 저장된다.

![images](https://github.com/hongyeongjune/kafka-playground/assets/39120763/3ef42985-44c5-4e42-8e53-8e1b15e1c7b2)

### Page Cache
* 카프카는 페이지 캐시를 이용한다.
* 카프카는 성능 향상을 위해 OS 페이지 캐시를 활용하는 방식으로 설계되어있다.
* 페이지 캐시는 직접 디스크를 읽고 쓰는 대신 물리 메모리 중 애플리케이션이 사용하지 않는 일부 잔여 메모리를 활용한다.
* 자세한 내용 : https://github.com/hongyeongjune/kafka-playground/blob/main/document/pageCache.md

### 배치 전송 처리
* 메시지를 쓰거나 읽을 때 개별 건마다 처리하지 않고 여러 건을 묶어서 일괄(batch) 처리하는 단순한 최적화를 통해 막대한 성능 개선 효과를 볼 수 있다.
* 일괄 처리는 한 번 전송에 사용되는 네트워크 패킷의 크기를 확대하므로 네트워크 I/O 오버헤드르 줄이고, 대규모의 순차 디스크 작업으로 이어지며, 연속적인 메모리 블록 사용으로 이어지는 등 효율 개선 효과가 매우 크다.

### 압축 전송
* 카프카는 메세지 전송 시 좀 더 성능이 높은 압축 전송을 사용하는 것을 권장한다.
* 카프카에서 지원하는 압축 타입은 gzip, snappy, lz4, zstd 등이 있다.
* 압축만으로도 네트워크 대역폭이나 회선 비용 등을 줄일 수 있는데, 위의 배치 전송 처리와 결합해 파일 하나를 압축하는 것 보단 여러개를 압축해 전송하는 것이 더 효율이 좋기 때문이다.

### Topic
* 카프카는 토픽(Topic)이라는 곳에 데이터를 저장한다.
* 토픽은 병렬 처리를 위해 여러개의 파티션(Partition)이라는 단위로 나뉜다.
* 파티션에 메세지가 저장되어 있는 위치를 오프셋(Offset)이라고 부르며, 오프셋은 순차적으로 증가하는 숫자 형태로 되어있다.
* 하나의 파티션 안에서의 오프셋은 고유한 숫자로, 카프카에서는 오프셋을 통해 메세지의 순서를 보장하고 컨슈머에서는 마지막까지 읽은 위치를 알 수 있다.

![images](http://cloudurable.com/images/kafka-architecture-topic-partition-layout-offsets.png)

### 고가용성 보장
* 카프카는 분산 시스템이기 때문에 하나의 서버나 노드가 다운되어도 다른 서버 또는 노드가 장애가 발생한 서버의 역할을 대신해 안정적인 서비스가 가능하다.
* 고가용성 보장을 위해 카프카에서는 리플리케이션(Replication)기능을 제공한다.
* 위의 설명과 같이 리플리케이션은 토픽 자체를 복제하는 것이 아닌 토픽의 파티션을 복제하는 것 이다.
* 원본과 리플리케이션을 구분하기 위해 리더와 팔로워라는 단어를 사용한다.
* 리플리케이션의 숫자가 무조건 많다고 좋은것은 아닌게 안정성은 높아지지만, 팔로워의 수만큼 결국 브로커의 공간이 소비되므로 이상적인 리플리케이션 팩터 수를 유지해야 한다.

### Producer 기본 동작

![images](https://jashangoyal.files.wordpress.com/2019/03/producer.png?w=810)

* 각 Record 들은 프로듀서의 send() 메서드를 통해 전달된다.
* 각 Record 에 파티션을 지정했다면 지정된 파티션으로 가고 지정하지 않았다면 키를 가지고 파티션을 선택해 Record 를 전달하는데, 기본적으로 라운도르빈 방식으로 동작한다.
* 프로듀서 내부에서는 send() 동작 이후 Record 들을 잠시 파티션별로 모아둔다. (배치 전송을 하기 위해서)
* 전송이 실패하면 재시도 지정 횟수만큼 재시도가 이뤄지고 최종적으로 실패하면 최종 실패를 전달하며 성공하면 메타데이터를 리턴한다.

### Consumer 기본 동작
* 프로듀서가 카프카의 토픽으로 메세지를 전송하면, 해당 메세지들은 브로커들의 로컬 디스크에 저장된다.
* 컨슈머는 토픽에 저장된 메세지를 가져올 수 있다.
* 컨슈머 그룹은 하나 이상의 컨슈머들이 모여 있는 그룹을 의미하고, 컨슈머는 반드시 컨슈머 그룹에 속하게 된다.
* 컨슈머 그룹은 각 파티션의 리더에게 카프카 토픽에 저장된 메세지를 가져오기 위한 요청을 보낸다.
* 이 떄 파티션의 수와 컨슈머 수는 일대일로 매핑되는 것이 이상적이다. (컨슈머 수가 파티션 수보다 많다고 해서 더 빠르게 토픽의 메세지를 가져오거나 처리량이 높아지는 것이 아닌 더 많은 수의 컨슈머들이 대기상태로 있기 때문)
* Active/Standby 개념으로 추가 컨슈머가 필요하다고 느낄 수 있지만, 컨슈머 그룹은 리밸런싱 동작을 통해 장애가 발생한 컨슈머의 역할을 동일한 그룹에 있는 다른 컨슈머가 그 역할을 대신 수행하므로 굳이 추가 리소스를 할당할 필요가 없다.

* 스프링 카프카에서 오토 커밋으로 설정하면 오프셋을 주기적으로 커밋하게 된다. 다만, 컨슈머 종료가 빈번하게 일어나면 일부 메세지를 못 가져오거나 중복으로 가져오는 경우가 생길 수 있다.
  * auto.commit.enable = true
  * auto.commit.interval.ms = 5000 (주기)
* 수동 커밋을 하면 AckMode 를 설정도 같이 해줘야한다.
  * 자세한 내용 : https://github.com/hongyeongjune/kafka-playground/blob/main/document/ackMode.md

### 컨슈머 그룹

* 컨슈머는 컨슈머 그룹 안에 속한 것이 일반적인 구조로, 하나의 컨슈머 그룹안에 여러 개의 컨슈머가 구성될 수 있다.
* 하나의 토픽에 여러개의 파티션이 있을 때, 컨슈머들은 파티션과 매칭되어 메세지를 가져오게된다.
* 따라서, 컨슈머의 개수가 아무리 많다고 한들 파티션의 개수가 적다면, 대기중인 컨슈머가 많아질 수 있다.

![images](https://www.oreilly.com/api/v2/epubs/9781491936153/files/assets/ktdg_04in05.png)
