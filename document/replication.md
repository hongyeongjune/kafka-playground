## Replication

### 간단한 개념
* 각 메세지들을 여러 개로 복제해서 카프카 클러스터내 브로커들에게 분산시키는 동작을 의미
* 리플리케이션 동작 덕분에 하나의 브로커가 종료되더라도 카프카는 안정성을 유지할 수 있다.
* 토핑 생성 명령어 중 replication-factor 라는 옵션으로 설정할 수 있다.
* 예를 들어, 특정 토픽의 replication-factor 를 3으로 설정했다면, 해당 토픽의 파티션이 브로커로 리플리케이션이 된다.
* 리플리케이션 팩터 수가 커지면 안정성은 높아지지만, 그만큼 브로커 리소스를 많이 사용하게 된다.

### 리더와 팔로워
* 카프카는 내부적으로 모두 동일한 리플리케이션(Replication)들을 리더와 팔로워로 구분하고, 각자의 역할을 분담한다.
* 리더
  * 리플리케이션 중 하나가 선정되며, 모든 읽기와 쓰기는 리더를 통해서만 가능하다.
  * 프로듀서가 특정 토픽으로 메세지를 전송하면, 파티션의 리더만 읽고, 쓰기가 가능하므로 파티션의 리더로 메세지를 보내고, 컨슈머에서도 리더로부터 메세지를 가져온다.
* 팔로워
  * 리더에 문제가 발생하거나 이슈가 있을 경우를 대비해 언제든지 새로운 리더가 될 준비를 한다.
  * 컨슈머가 토픽의 메세지를 꺼내 가는 것과 비슷한 동작으로 지속적으로 파티션의 리더가 새로운 메세지를 받았는지 확인하고 새로운 메세지가 있다면 복제한다.

### 복제 유지와 커밋
* 리더와 팔로워는 ISR(InSyncReplica)라는 논리적 그룹으로 묶여있다.
* ISR 그룹 안에 속한 팔로워들만이 새로운 리더의 자격을 가질 수 있다.
* 리더는 팔로워들이 뒤쳐지지 않고 리플리케이션을 잘하고 있는지를 감시한다. -> 리플리케이션을 잘 못한다면, 리더가 ISR 그룹에서 방출시킨다.
  * 예시로, 팔로워의 네트워크 오류, 팔로워 브로커 장애, 팔로워가 특정 주기의 시간만큼 복제 요청을 하지 않는 경우 등에 방출
* ISR 내에서 모든 팔로워의 복제가 완료되면, 리더는 내부적으로 커밋되었다는 표시를 하게 된다.
* 마지막 커밋 오프셋 위치는 **하이워터마크(High Watermark)** 라고 부른다.
* 그리고, 이렇게 커밋된 메세지만 컨슈머가 읽을 수 있다.
* 모든 브로커는 재시작될 때, 커밋된 메세지를 유지하기 위해 **로컬 디스크의 replication-offset-checkpoint** 라는 파일에 마지막 커밋 오프셋 위치를 저장한다.

### 리더와 팔로워의 리플리케이션 동작
* 팔로워들은 주기적으로 리더에게 메세지 **가져오기 요청(Fetch Request)** 을 보낸다.
* 가져오기 요청의 응답에는 **Offset, Leader Epoch, 하이워터마크** 가 포함되어 있다.
* 리더는 팔로워들이 메세지를 가져갔을 때 현재 가져간 메세지가 복제에 성공했는지 안했는지에 대해서는 모른다.
  * Rabbit MQ 의 트랜잭션 모드에서는 모든 미러(카프카로 치면 팔로워)가 메세지를 받았는지 ACK 를 리더에게 리턴하므로, 리더는 미러들이 메세지를 받았는지 알 수 있다.
  * 하지만, 카프카는 리더와 팔로워 간 ACK 를 주고받는 통신은 없다. -> 성능이 더 빠르다.
* 팔로워는 특정 메세지 복제를 완료했으면 다음 메세지를 또 요청한다. 이 때 리더는 가져오기 요청이 온 메세지 이전까지의 메세지는 복제가 다 완료되었다고 인지하고, 이전 메세지까지 커밋하고 하이워터마크를 1 증가시킨다.
* 결론은, 팔로워들이 Pull 방식으로 리더의 데이터를 가져가는 방식을 채택하면서 리더의 부하를 줄여 성능을 높였다.

### acks
> https://learn.conduktor.io/kafka/kafka-topic-replication/#Kafka-producers-acks-setting-3

* 프로듀서가 카프카 토픽의 리더에게 메시지를 보낸 후 요청 완료 전 ACK 를 받아야 하는 수를 의미한다. 옵션의 수가 작을 수록 성능이 좋아지지만 메시지 손실 가능성이 높아진다.
* acks 는 0, 1, all 세 가지 옵션으로 설정할 수 있다.
* acks = 0
  * 프로듀서는 브로커가 메시지를 수락할 때까지 기다리지 않고 메시지를 전송한 순간 메시지를 성공적으로 작성된 것으로 간주한다.
* acks = 1
  * 프로듀서는 메시지가 리더에게만 승인되면 성공적으로 작성된 메시지로 간주한다.
* acks = all
  * 프로듀서는 모든 동기화 복제본(ISR)이 메시지를 수락하면 메시지를 성공적으로 작성된 것으로 간주한다.

### min.insync.replicas
> https://docs.confluent.io/platform/current/installation/configuration/topic-configs.html#min-insync-replicas

* 프로듀서가 메세지를 보낼 때, 쓰기를 성공하기 위한 최소한의 복제본 수를 의미힌다.
* 즉, ACK 응답을 보내기 위한 리더가 확인해야할 최소 리플리케이션의 수를 지정하는 브로커 관련 옵션이다.
* 근데, acks=all 로 설정했다고해도, min.insync.replicas 가 1 이라면, 리더만 메세지를 수신해도 리플리케이션의 조건이 충족되기 때문에 ACK 응답을 보내게 된다.
* 또한, acks=all 일 때 min.insync.replicas 를 replication factor 수와 똑같이 한다면, 브로커 하나가 장애 났을 때 가용 가능한 브로커 수는 min.insync.replicas 개수 보다 하나가 적게 될 텐데 이 때, 메세지를 수신하면 replication factor 수가 적기 때문에 무조건 에러가 발생한다.
* 따라서, acks=all 이면, min.insync.replicas 는 2로 설정하는 것이 안정적이고, 성능도 좋다.

### 리더에포크와 복구
* 리더에포크(Leader Epoch)는 카프카의 파티션들이 복구 동작을 할 때, 메세지의 일관성을 유지하기 위한 용도로 사용된다.
* 리더에포크는 컨트롤러에 의해 관리되는 32비트의 숫자
* 리더에포크 정보는 리플리케이션 프로토콜에 의해 전파(정확히는 팔로워가 가져오기 요청을 하면 응답으로 리더 에포크를 전달한다.)
  * 상세 이미지 https://developer.confluent.io/courses/architecture/data-replication/#follower-fetch-response
* 리더 에포크는 리더가 장애가 발생했을 때만 변화가 있다.
* 리더 에포크는 고유의 번호이다.

## 생각정리
### 상황
* 파티션 수 : 1
* replication factor : 2
* acks = all
* min.insync.replicas : 1

### ISR 내에서 모든 복제가 완료되면, 리더가 내부적으로 커밋을 한다. 이 마지막 커밋을 하이워터마크라고 하는데, min.insync.replicas 가 1이라면 복제와 상관없이 리더만 잘 받았다면 내부적으로 커밋이 되는건지 ?
* min.insync.replicas 가 1 이라면 ISR 내세어 리더만 데이터 수신에 성공하면 프로듀서에게 ACK 응답을 보낸다.
* 하이워터마크는 모든 팔로워가 복제에 성공해야만 올라간다.

### 그럼 하이워터마크는 올라가지 않았지만, min.insync.replicas 를 낮춰서 프로듀서에게 ACK 응답을 빠르게 보냈을 때 얻어지는 이점은 무엇인가 ?
* 데이터 손실을 최소화할 수 있다. -> 프로듀서에게 ACK 가 반환되면 프로듀서는 메세지가 적어도 리더에 저장되었음을 신뢰할 수 있다.
* ISR 재구성 시에도 팔로워 중 하나가 장애가 나도 리더는 ISR 을 재구성하면서 자신만으로 데이터를 안전하게 복구할 수 있다.
* min.insync.replicas 를 1로 권장하지 않는 이유는 리더가 장애났을 때 데이터 유실이 될 수 있기 때문에, 2로 해서 팔로워 한개 까지 안전 복제 하는게 좋다.
* 팔로워 복제 여부와 상관없이 min.insync.replicas 조건만 만족하면 프로듀서가 빠르게 응답을 받을 수 있어서 성능적으로도 좋다.

### 그럼 팔로워가 많을 수록 소비되는 속도가 많이 느려지는가 ?
* 하이워터마크는 팔로워의 복제 완료를 기준으로 한다.
* 따라서 팔로워가 많으면 복제하는데 오래걸려서 속도가 느려질 가능성이 있다.

### 하이워터마크가 올라가는 시점은 모든 팔로워가 복제가 완료된 시점인데, 모든 팔로워가 복제를 완료했다는걸 리더가 어떻게 아는지 ?
* 팔로워는 리더에게 주기적으로 가져오기 요청을 보낸다.
* 리더는 가져오기 요청을 수신하면 팔로워가 요청한 Offset 을 보고 이전까지는 복제를 완료했다고 인지하고 ISR 내 상태를 업데이트한다. (하이워터마크증가)
  * 결국 모든 팔로워가 가져오기 요청을 보냄으로써 특정 오프셋에 도달했는지 확인하고, 모든 ISR 멤버가 동일한 오프셋에 도달해야 하이워터마크가 올라간다.
* 또한, 리더는 팔로워가 일정 시간 내에 복제를 못하면 해당 팔로워는 ISR 에서 제거된다.
* replica.lag.max.messages 같은 옵션으로 팔로워가 리더에게 너무 많이 뒤쳐지는 것을 방지할 수 있음
* fetch.max.wait.ms 옵션으로 팔로워가 리더에서 데이터를 가져오는 주기를 조정하여 복제 속도를 최적화할 수 있음
* 참고 : https://levelup.gitconnected.com/high-water-mark-hwm-in-kafka-offsets-5593025576ac

### 하이워터마크가 올라가면 팔로워들은 올라간 하이워터마크를 어떻게 알까 ?
* ISR 내 모든 팔로워가 동일한 오프셋에 올라가는게 확인되면 하이워터마크를 1 올리는 작업을 시작한다.
* 하이워터마크가 1 올라가면 그 즉시, 리더가 팔로워들한테 전파하는 것이 아니다.
* 팔로워는 주기적으로 Fetch 요청을 보내면서 데이터를 가져온다.
* 리더는 Fetch 요청에 대한 응답에 현재의 하이워터마크 값을 포함하여 팔로워에게 전달한다.
* 이 과정에서 하이워터마크 정보는 팔로워가 요청을 보낸 시점의 상태에 기반하여 전달하게된다.
* 즉, 리더가 하이워터마크를 갱신한 시점과 팔로워가 이를 알게되는 시점은 다를 수 있다.

### 마지막 점검으로 팔로워가 Offset 2 를 가져오기 요청했다고 하면, 리더는 Offset 1 까지 복제가 완료되었다고 인지하고 하이워터마크를 1로 설정하는 건가 ?
* 아니다, 모든 ISR 의 복제가 완료되어야한다.
* 즉, 특정 팔로워가 Offset 2 를 가져오기 요청을 해도 다른 팔로워가 아직 요청을 안했으면 하이워터마크를 올릴 수 없다.
* 다만, 다른 모든 팔로워가 Offset 2 가져오기 요청을 했고, 마지막 남은 하나가 Offset 2 가져오기 요청을 안했다면, 리더는 하이워터마크를 올릴 수 있다.

### 리더 에포크는 그래서 역할이 뭐지 ?
* 리더 에포크는 데이터 무결성 보장을 위한 용도로 사용된다.
* 리더 에포크는 특정 파티션의 리더가 변경될 때마다 증가하는 숫자 값이다.
* 새로운 리더가 선출되면, 리더 에포크가 증가하며 새로운 리더의 유효성을 나타내는 고유한 식별자 역할을 한다.
* 모든 클라이언트와 팔로워는 리더 에포크를 기준으로 "현재의 유효한 리더"를 판단할 수 있다.
* 장애가 발생한 기존 리더가 다시 살아날 수도 있지만, 기존 리더가 이미 유효하지 않다는 것을 보장하기 위해 리더 에포크를 사용한다.
* 새로운 리더의 에포크가 더 크기 때문에, 모든 팔로워와 클라이언트는 새 리더만을 신뢰하고 기존 리더로부터의 혼란을 방지합니다.
  * 리더 A의 에포크: 5
  * 리더 B가 새로운 리더로 선출됨 → 에포크: 6
  * 리더 A가 복구되더라도 에포크 5의 리더로 동작할 수 없음.
* 클라이언트가 데이터 쓰기(Produce) 또는 읽기(Fetch) 요청을 보낼 때, 리더 에포크를 확인한다.
* 클라이언트는 요청을 보낸 리더가 여전히 유효한 리더인지 에포크를 기준으로 판단한다.
* 만약 클라이언트가 오래된 리더(낮은 에포크)에 요청을 보낸 경우, 새로운 리더로 재시도하도록 유도된다.
* 클라이언트가 에포크 5의 리더 A에게 요청을 보냈지만, 이 리더가 이미 장애 상태라면, 에포크 6의 새로운 리더 B로 요청을 재전송한다.

### 리더가 장애가 나서 복구하는 과정에 리더의 Offset 과 팔로워의 Offset 간 차이가 있고, 하이워터마크 간 차이도 있으면 리더의에 있던 Offset 은 사라지는가?
* 리더는 오프셋 5번 까지 저장했고, 현재 하이워터마크가 3이라고 가정했을 때 리더가 장애가 났다.
* 그럼 리더의 오프셋 4번, 오프셋 5번은 복제를 하지 못한 상태로 팔로워가 새로운 리더로 선출될 것 이다.
* 그러면 오프셋 4번, 오프셋 5번은 사라지게 된다. 왜냐면 카프카는 데이터 무결성 보장"을 우선하며, 커밋되지 않은 데이터를 손실하는 것을 허용하기 때문이다.
* 다만, min.insync.replicas 를 2로 설정한 상황에서 만약 리더가 장애가 났다면, 프로듀서는 ACK 응답을 받지 못했음으로 재시도를 할 수 있다.
* 따라서, 이때는 오프셋 4번, 오프셋 5번을 프로듀서가 다시 보냄으로써 데이터 손실을 방지할 수 있다.

